{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "from itertools import combinations, permutations\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    ----------\n",
    "    data_raw : pd.DataFrame\n",
    "        rawデータ\n",
    "    data_preprocessed : pd.DataFrame\n",
    "        preprocessing後のデータ\n",
    "    data_horse_result_merged : pd.DataFrame\n",
    "        merge_horse_results後のデータ\n",
    "    data_peds_merged : pd.DataFrame\n",
    "        merge_peds後のデータ\n",
    "    data_categorical_processed : pd.DataFrame\n",
    "        process_categorical後のデータ\n",
    "    no_peds: Numpy.array\n",
    "        merge_pedsを実行した時に、血統データが存在しなかった馬のhorse_id一覧\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data_raw = pd.DataFrame()\n",
    "        self.data_preprocessed = pd.DataFrame()\n",
    "        self.data_horse_result_merged = pd.DataFrame()\n",
    "        self.data_peds_merged = pd.DataFrame()\n",
    "        self.data_categorical_processed = pd.DataFrame()\n",
    "\n",
    "    def merge_horse_results(self, hr, n_samples_list=[5, 9, \"all\"]):\n",
    "        \"\"\"\n",
    "        馬の過去成績データから、\n",
    "        n_samples_listで指定されたレース分の着順と賞金の平均を追加してdata_hに返す\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        hr : HorseResults\n",
    "            馬の過去成績データ\n",
    "        n_samples_list : list, default [5, 9, 'all']\n",
    "            過去何レース分追加するか\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_horse_result_merged = self.data_preprocessed.copy()\n",
    "        for n_samples in n_samples_list:\n",
    "            self.data_horse_result_merged = hr.merge_all(self.data_horse_result_merged, n_samples=n_samples)\n",
    "\n",
    "        # 6/6追加： 馬の出走間隔追加\n",
    "        self.data_horse_result_merged[\"interval\"] = (self.data_horse_result_merged[\"date\"] - self.data_horse_result_merged[\"latest\"]).dt.days\n",
    "        self.data_horse_result_merged.drop([\"開催\", \"latest\"], axis=1, inplace=True)\n",
    "\n",
    "    def merge_peds(self, peds):\n",
    "        \"\"\"\n",
    "        5世代分血統データを追加してdata_peに返す\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        peds : Peds.peds_e\n",
    "            Pedsクラスで加工された血統データ。\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_peds_merged = self.data_horse_result_merged.merge(\n",
    "            peds, left_on=\"horse_id\", right_index=True, how=\"left\"\n",
    "        )\n",
    "        self.no_peds = self.data_peds_merged[self.data_peds_merged[\"peds_0\"].isnull()][\n",
    "            \"horse_id\"\n",
    "        ].unique()\n",
    "        if len(self.no_peds) > 0:\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "\n",
    "    def process_categorical(self, le_horse, le_jockey, results_m):\n",
    "        \"\"\"\n",
    "        カテゴリ変数を処理してdata_cに返す\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        le_horse : sklearn.preprocessing.LabelEncoder\n",
    "            horse_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "        le_jockey : sklearn.preprocessing.LabelEncoder\n",
    "            jockey_idを0始まりの整数に変換するLabelEncoderオブジェクト。\n",
    "        results_m : Results.data_pe\n",
    "            ダミー変数化のとき、ResultsクラスとShutubaTableクラスで列を合わせるためのもの\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.data_peds_merged.copy()\n",
    "\n",
    "        # ラベルエンコーディング。horse_id, jockey_idを0始まりの整数に変換\n",
    "        mask_horse = df[\"horse_id\"].isin(le_horse.classes_)\n",
    "        new_horse_id = df[\"horse_id\"].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df[\"horse_id\"] = le_horse.transform(df[\"horse_id\"])\n",
    "        mask_jockey = df[\"jockey_id\"].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df[\"jockey_id\"].mask(mask_jockey).dropna().unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df[\"jockey_id\"] = le_jockey.transform(df[\"jockey_id\"])\n",
    "\n",
    "        # horse_id, jockey_idをpandasのcategory型に変換\n",
    "        df[\"horse_id\"] = df[\"horse_id\"].astype(\"category\")\n",
    "        df[\"jockey_id\"] = df[\"jockey_id\"].astype(\"category\")\n",
    "\n",
    "        # そのほかのカテゴリ変数をpandasのcategory型に変換してからダミー変数化\n",
    "        # 列を一定にするため\n",
    "        weathers = results_m[\"weather\"].unique()\n",
    "        race_types = results_m[\"race_type\"].unique()\n",
    "        ground_states = results_m[\"ground_state\"].unique()\n",
    "        sexes = results_m[\"性\"].unique()\n",
    "        df[\"weather\"] = pd.Categorical(df[\"weather\"], weathers)\n",
    "        df[\"race_type\"] = pd.Categorical(df[\"race_type\"], race_types)\n",
    "        df[\"ground_state\"] = pd.Categorical(df[\"ground_state\"], ground_states)\n",
    "        df[\"性\"] = pd.Categorical(df[\"性\"], sexes)\n",
    "        df = pd.get_dummies(df, columns=[\"weather\", \"race_type\", \"ground_state\", \"性\"])\n",
    "\n",
    "        self.data_categorical_processed = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(old, new):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    old : pandas.DataFrame\n",
    "        古いデータ\n",
    "    new : pandas.DataFrame\n",
    "        新しいデータ\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old, new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "        # ネット競馬プレミアムのログイン情報\n",
    "        self.user_id = \"ken.osechi@gmail.com\"\n",
    "        self.password = \"BQj4WGwzLxJZNkWiw\"\n",
    "        self.login_info = {\n",
    "            \"login_id\": self.user_id,\n",
    "            \"pswd\": self.password,\n",
    "        }\n",
    "        self.session = requests.session()\n",
    "        self.url_login = \"https://regist.netkeiba.com/account/?pid=login&action=auth\"\n",
    "        self.ses = self.session.post(self.url_login, data=self.login_info)\n",
    "        self.cookie = self.ses.cookies\n",
    "\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "\n",
    "    @staticmethod\n",
    "    def scrape(self, race_id_list):\n",
    "        \"\"\"\n",
    "        レース結果データをスクレイピングする関数\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        race_id_list : list\n",
    "            レースIDのリスト\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        race_results_df : pandas.DataFrame\n",
    "            全レース結果データをまとめてDataFrame型にしたもの\n",
    "        \"\"\"\n",
    "\n",
    "        # race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                url = \"https://db.sp.netkeiba.com/race/\" + race_id\n",
    "\n",
    "                # html = requests.get(url)\n",
    "                # html.encoding = \"EUC-JP\"\n",
    "\n",
    "                html = self.session.get(url, cookies=self.cookie)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "\n",
    "                # メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(html.text)[0]\n",
    "                # 列名に半角スペースがあれば除去する\n",
    "                df = df.rename(columns=lambda x: x.replace(\" \", \"\"))\n",
    "\n",
    "                # 天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"data_intro\"})\n",
    "                    .find_all(\"p\")[0]\n",
    "                    .text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"data_intro\"})\n",
    "                    .find_all(\"p\")[1]\n",
    "                    .text\n",
    "                )\n",
    "                info = re.findall(r\"\\w+\", texts)\n",
    "                for text in info:\n",
    "                    if text in [\"芝\", \"ダート\"]:\n",
    "                        df[\"race_type\"] = [text] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[-1])] * len(\n",
    "                            df\n",
    "                        )  \n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "                    if \"年\" in text:\n",
    "                        df[\"date\"] = [text] * len(df)\n",
    "\n",
    "                # 馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/horse\")}\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\n",
    "                    \"a\", attrs={\"href\": re.compile(\"^/jockey\")}\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                # インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "            # 存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except AttributeError:  # 存在しないrace_idでAttributeErrorになるページもあるので追加\n",
    "                continue\n",
    "            # wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            # Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "\n",
    "    # 前処理\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df[\"着順\"] = pd.to_numeric(df[\"着順\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"着順\"], inplace=True)\n",
    "        df[\"着順\"] = df[\"着順\"].astype(int)\n",
    "        df[\"rank\"] = df[\"着順\"].map(lambda x: 1 if x < 4 else 0)\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0]\n",
    "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
    "\n",
    "        # errors='coerce'で、\"計不\"など変換できない時に欠損値にする\n",
    "        df[\"体重\"] = pd.to_numeric(df[\"体重\"], errors=\"coerce\")\n",
    "        df[\"体重変化\"] = pd.to_numeric(df[\"体重変化\"], errors=\"coerce\")\n",
    "\n",
    "        # 単勝をfloatに変換\n",
    "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "        # 距離は10の位を切り捨てる\n",
    "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df.drop(\n",
    "            [\"タイム\", \"着差\", \"調教師\", \"性齢\", \"馬体重\", \"馬名\", \"騎手\", \"人気\", \"着順\"],\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y年%m月%d日\")\n",
    "\n",
    "        # 開催場所\n",
    "        df[\"開催\"] = df.index.map(lambda x: str(x)[4:6])\n",
    "\n",
    "        # 6/6出走数追加\n",
    "        df[\"n_horses\"] = df.index.map(df.index.value_counts())\n",
    "\n",
    "        self.data_p = df\n",
    "\n",
    "    # カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncoder().fit(self.data_peds_merged[\"horse_id\"])\n",
    "        self.le_jockey = LabelEncoder().fit(self.data_peds_merged[\"jockey_id\"])\n",
    "        super().process_categorical(\n",
    "            self.le_horse, self.le_jockey, self.data_peds_merged\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RaceResultsSP:\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.user_id = \"ken.osechi@gmail.com\"\n",
    "        self.password = \"BQj4WGwzLxJZNkWiw\"\n",
    "        self.login_info = {\n",
    "            \"login_id\": self.user_id,\n",
    "            \"pswd\": self.password,\n",
    "        }\n",
    "        self.session = requests.session()\n",
    "        self.url_login = \"https://regist.netkeiba.com/account/?pid=login&action=auth\"\n",
    "        self.ses = self.session.post(self.url_login, data=self.login_info)\n",
    "        self.cookie = self.ses.cookies\n",
    "\n",
    "    def scrape(self, race_id_list):\n",
    "        # race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        odds_results = {}\n",
    "        lap_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                url = \"https://db.sp.netkeiba.com/race/\" + race_id\n",
    "                html = self.session.get(url, cookies=self.cookie)\n",
    "                html.encoding = \"EUC-JP\"\n",
    "                soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "                race_data = soup.find(\"div\", class_=\"RaceData\").find_all(\"span\")\n",
    "                race_data = \"\".join(tag.get_text() for tag in race_data)\n",
    "\n",
    "                # htmlからBeautifulSoupでデータを抽出\n",
    "                df = pd.read_html(html.text)[0]\n",
    "                df = df.rename(columns=lambda x: x.replace(\" \", \"\"))  # カラム名の空白削除\n",
    "\n",
    "                # 天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "                texts = (\n",
    "                    soup.find(\"div\", attrs={\"class\": \"RaceHeader_Value\"}).text\n",
    "                    + soup.find(\"div\", attrs={\"class\": \"RaceHeader_Value_Others\"}).text\n",
    "                )\n",
    "\n",
    "                date = soup.find(\"span\", attrs={\"class\": \"Race_Date\"}).text.strip()\n",
    "                df[\"date\"] = [date] * len(df)\n",
    "\n",
    "                race_info = re.findall(r\"\\w+\", texts)\n",
    "\n",
    "                for text in race_info:\n",
    "                    if \"芝\" in text:\n",
    "                        df[\"race_type\"] = [\"芝\"] * len(df)\n",
    "                    if \"ダート\" in text:\n",
    "                        df[\"race_type\"] = [\"ダート\"] * len(df)\n",
    "                    if \"障\" in text:\n",
    "                        df[\"race_type\"] = [\"障害\"] * len(df)\n",
    "                    if \"m\" in text:\n",
    "                        df[\"course_len\"] = [int(re.findall(r\"\\d+\", text)[-1])] * len(df)\n",
    "                    if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                        df[\"ground_state\"] = [text] * len(df)\n",
    "                        # 馬場指数\n",
    "                        if \"障\" in text:\n",
    "                            df[\"ground_index\"] = [0] * len(df)\n",
    "                        else:\n",
    "                            baba_index = pd.read_html(html.text)[2].iloc[0, 1]\n",
    "                            df[\"ground_index\"] = [baba_index] * len(df)\n",
    "                    if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:\n",
    "                        df[\"weather\"] = [text] * len(df)\n",
    "\n",
    "                # horse_id\n",
    "                horse_id_list = []\n",
    "                horse_a_list = (\n",
    "                    soup.find(\n",
    "                        \"table\", attrs={\"class\": \"table_slide_body ResultsByRaceDetail\"}\n",
    "                    )\n",
    "                    .find_all(\"tbody\")[0]\n",
    "                    .find_all(\"a\", attrs={\"href\": re.compile(r\"horse/(\\d+)/\")})\n",
    "                )\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = (\n",
    "                    soup.find(\n",
    "                        \"table\", attrs={\"class\": \"table_slide_body ResultsByRaceDetail\"}\n",
    "                    )\n",
    "                    .find_all(\"tbody\")[0]\n",
    "                    .find_all(\"a\", attrs={\"href\": re.compile(r\"jockey/(\\d+)/\")})\n",
    "                )\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "\n",
    "                df[\"horse_id\"] = horse_id_list\n",
    "                df[\"jockey_id\"] = jockey_id_list\n",
    "\n",
    "                # indexをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "                race_results[race_id] = df\n",
    "\n",
    "                # オッズの取得\n",
    "                # odds = pd.read_html(html.text)[1]\n",
    "                # type = odds.iloc[:, 0]\n",
    "                # win = odds.iloc[:, 2]\n",
    "                # odds_df = pd.concat([type, win], axis=1).set_index(0).T\n",
    "                # odds_df.index = [race_id]\n",
    "                # odds_results[race_id] = odds_df\n",
    "\n",
    "                # laptimeの取得\n",
    "                len_ls = (\n",
    "                    [i for i in range(100, 2600, 100)]\n",
    "                    + [i + 50 for i in range(100, 1200, 100)]\n",
    "                    + [i for i in range(2600, 3601, 200)]\n",
    "                )\n",
    "                len_ls = sorted(len_ls)\n",
    "                len_ls = [f\"{i}m\" for i in len_ls]\n",
    "                lap_df = pd.DataFrame(columns=len_ls)\n",
    "                if \"障\" in race_data:\n",
    "                    lap_df.loc[0] = np.nan\n",
    "                else:\n",
    "                    lap_time_read = pd.read_html(html.text)[5]\n",
    "                    lap_time_read = lap_time_read.T.reset_index().T.reset_index(\n",
    "                        drop=True\n",
    "                    )\n",
    "\n",
    "                    for i in range(lap_time_read.shape[0]):\n",
    "                        if i % 2 == 1:\n",
    "                            pass\n",
    "                        else:\n",
    "                            for j in range(lap_time_read.shape[1]):\n",
    "                                if pd.isna(lap_time_read.iloc[i, j]):\n",
    "                                    break\n",
    "                                length = str(lap_time_read.iloc[i, j])\n",
    "                                lap_df[length] = [\n",
    "                                    float(lap_time_read.iloc[i + 1, j].split()[1])\n",
    "                                ]\n",
    "                lap_df.index = [race_id]\n",
    "                lap_results[race_id] = lap_df\n",
    "\n",
    "            # 存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except AttributeError:  # 存在しないrace_idでAttributeErrorになるページもあるので追加\n",
    "                continue\n",
    "            # wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            # Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "        # odds_results_df = pd.concat([odds_results[key] for key in odds_results], ignore_index=True)\n",
    "        lap_results_df = pd.concat([lap_results[key] for key in lap_results])\n",
    "\n",
    "        return race_results_df, lap_results_df\n",
    "\n",
    "    def preprocess(self, race_res):\n",
    "        df = race_res.copy()\n",
    "\n",
    "        df[\"着順\"] = pd.to_numeric(df[\"着順\"], errors=\"coerce\")\n",
    "        df.dropna(subset=[\"着順\"], inplace=True)\n",
    "        df[\"着順\"] = df[\"着順\"].astype(int)\n",
    "        df[\"rank\"] = df[\"着順\"].map(lambda x: 1 if x < 6 else 0)\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df[\"性\"] = df[\"性齢\"].map(lambda x: str(x)[0])\n",
    "        df[\"年齢\"] = df[\"性齢\"].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df[\"体重\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[0]\n",
    "        df[\"体重変化\"] = df[\"馬体重\"].str.split(\"(\", expand=True)[1].str[:-1]\n",
    "\n",
    "        # errors='coerce'で、\"計不\"など変換できない時に欠損値にする\n",
    "        df[\"体重\"] = pd.to_numeric(df[\"体重\"], errors=\"coerce\")\n",
    "        df[\"体重変化\"] = pd.to_numeric(df[\"体重変化\"], errors=\"coerce\")\n",
    "\n",
    "        # 単勝をfloatに変換\n",
    "        df[\"単勝\"] = df[\"単勝\"].astype(float)\n",
    "        # 距離は10の位を切り捨てる\n",
    "        df[\"course_len\"] = df[\"course_len\"].astype(float) // 100\n",
    "\n",
    "        def convert_time_to_seconds(time_str):\n",
    "            # タイムの形式が '分:秒.ミリ秒' と仮定\n",
    "            # 分と秒を分離\n",
    "            minutes, seconds = time_str.split(\":\")\n",
    "            # 分を秒に変換し、秒と合算\n",
    "            return int(minutes) * 60 + float(seconds)\n",
    "\n",
    "        # 'タイム' 列の各エントリに対して変換関数を適用\n",
    "        df[\"タイム\"] = df[\"タイム\"].apply(convert_time_to_seconds)\n",
    "\n",
    "        # def convert_margin_to_number(margin):\n",
    "        #     if pd.isna(margin):\n",
    "        #         return 0\n",
    "        #     if margin == \"同着\":\n",
    "        #         return 0\n",
    "        #     if margin == \"ハナ\":\n",
    "        #         return 0.2\n",
    "        #     if margin == \"アタマ\":\n",
    "        #         return 0.4\n",
    "        #     if margin == \"クビ\":\n",
    "        #         return 0.8\n",
    "        #     if margin == \"大\":\n",
    "        #         return 10\n",
    "        #     # その他の値を数値に変換\n",
    "        #     total = 0\n",
    "\n",
    "        #     parts = margin.split(\".\")\n",
    "        #     if len(parts) == 2:\n",
    "        #         total += float(parts[0])\n",
    "        #         fraction = parts[1].split(\"/\")\n",
    "        #         if len(fraction) == 2:\n",
    "        #             total += float(fraction[0]) / float(fraction[1])\n",
    "\n",
    "        #     else:\n",
    "        #         fraction = margin.split(\"/\")\n",
    "        #         if len(fraction) == 2:\n",
    "        #             total += float(fraction[0]) / float(fraction[1])\n",
    "        #         else:\n",
    "        #             total += float(margin)\n",
    "        #     return total * 2.8\n",
    "\n",
    "        # '着差' 列の各エントリに変換関数を適用し、2.8を掛ける\n",
    "        # df[\"着差_数値\"] = df[\"着差\"].apply(convert_margin_to_number)\n",
    "        # 累積合計を計算\n",
    "        # df[\"着差累積\"] = df[\"着差_数値\"].cumsum()\n",
    "\n",
    "        df[\"調教場所\"] = df[\"調教師\"].map(lambda x: str(x)[1:2])\n",
    "        df[\"調教師名前\"] = df[\"調教師\"].map(lambda x: str(x)[3:])\n",
    "\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"].str.split(\"(\").str[0])\n",
    "\n",
    "        # 通過の列を分割する関数\n",
    "        def split_pass_columns(pass_str):\n",
    "            # 通過位置を'-'で分割し、最大4つの位置まで取得する\n",
    "            if \"-\" not in str(pass_str):\n",
    "                return [pass_str]\n",
    "            parts = pass_str.split(\"-\") + [None] * (4 - len(pass_str.split(\"-\")))\n",
    "            return parts[:4]\n",
    "\n",
    "        # 新しい列を作成\n",
    "        df[[\"通過1\", \"通過2\", \"通過3\", \"通過4\"]] = df[\"通過\"].apply(\n",
    "            lambda x: pd.Series(split_pass_columns(x))\n",
    "        )\n",
    "\n",
    "        df.drop(\n",
    "            [\"着差\", \"通過\", \"調教師\", \"性齢\", \"馬体重\", \"馬名\", \"騎手\", \"人気\", \"着順\", \"調教タイム\", \"厩舎コメント\"],\n",
    "            axis=1,\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_res = RaceResultsSP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8640"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2021\n",
    "race_id_list = []\n",
    "for place in range(1, 11, 1):\n",
    "    for kai in range(1, 7, 1):\n",
    "        for day in range(1, 13, 1):\n",
    "            for r in range(1, 13, 1):\n",
    "                race_id = (\n",
    "                    str(year)\n",
    "                    + str(place).zfill(2)\n",
    "                    + str(kai).zfill(2)\n",
    "                    + str(day).zfill(2)\n",
    "                    + str(r).zfill(2)\n",
    "                )\n",
    "                race_id_list.append(race_id)\n",
    "len(race_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32114e2aaa5478785ed49325379c71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94f28f3d2b6449eaa2474095aaae1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b2f8e4713c44ad92a6e066cda09dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c16b29af7714bfe86233066c895908d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5047d5996f14042865636df71eb84b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/KeD/Scripts/python/keiba/Scrape.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(race_id_list), \u001b[39m1000\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     a, b \u001b[39m=\u001b[39m race_res\u001b[39m.\u001b[39;49mscrape(race_id_list[i : i \u001b[39m+\u001b[39;49m \u001b[39m1000\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     df_a \u001b[39m=\u001b[39m race_res\u001b[39m.\u001b[39mpreprocess(a)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     df_a\u001b[39m.\u001b[39mto_pickle(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/Users/KeD/Scripts/python/keiba/KeibaAI/keiba/RaceRes/Race_\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1000\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     )\n",
      "\u001b[1;32m/Users/KeD/Scripts/python/keiba/Scrape.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m \u001b[39m# pd.DataFrame型にして一つのデータにまとめる\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m race_results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([race_results[key] \u001b[39mfor\u001b[39;49;00m key \u001b[39min\u001b[39;49;00m race_results])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m \u001b[39m# odds_results_df = pd.concat([odds_results[key] for key in odds_results], ignore_index=True)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/KeD/Scripts/python/keiba/Scrape.ipynb#X31sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m lap_results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([lap_results[key] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m lap_results])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:369\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    149\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    160\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39m    1   3   4\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    370\u001b[0m         objs,\n\u001b[1;32m    371\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    372\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    373\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    374\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    375\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    376\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    377\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    378\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    379\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    380\u001b[0m     )\n\u001b[1;32m    382\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/concat.py:426\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    423\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(race_id_list), 1000):\n",
    "    a, b = race_res.scrape(race_id_list[i : i + 1000])\n",
    "    df_a = race_res.preprocess(a)\n",
    "    \n",
    "    df_a.to_pickle(\n",
    "        f\"/Users/KeD/Scripts/python/keiba/KeibaAI/keiba/RaceRes/Race_{year}_{i}-{i+1000}.pickle\"\n",
    "    )\n",
    "    b.to_pickle(\n",
    "        f\"/Users/KeD/Scripts/python/keiba/KeibaAI/keiba/RaceRes/Lap_{year}_{i}-{i+1000}.pickle\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = race_res.preprocess(a)\n",
    "df_a.to_pickle(\n",
    "    f\"/Users/KeD/Scripts/python/keiba/KeibaAI/keiba/RaceRes/Race_2023_{i}-{i+1000}.pickle\"\n",
    ")\n",
    "b.to_pickle(\n",
    "    f\"/Users/KeD/Scripts/python/keiba/KeibaAI/keiba/RaceRes/Lap_2023_{i}-{i+1000}.pickle\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3083"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
